// INSTRUCTION: Please remove all comments that start INSTRUCTION prior to commit. Most comments should be removed, although not the copyright.
// INSTRUCTION: The copyright statement must appear at the top of the file
//
// Copyright (c) 2017 IBM Corporation and others.
// Licensed under Creative Commons Attribution-NoDerivatives
// 4.0 International (CC BY-ND 4.0)
//   https://creativecommons.org/licenses/by-nd/4.0/
//
// Contributors:
//     IBM Corporation
//
:projectid: kubernetes
:page-layout: guide
:page-duration: 30 minutes
:page-releasedate: 2018-05-30
:page-description: Explore how to deploy microservices to Kubernetes using Helm charts and manage them with kubectl.
:page-tags: ['microservices', 'Helm', '{kube}', 'Docker', 'containers', 'kubectl', 'Minikube']
:page-permalink: /guides/{projectid}
:page-related-guides: ['docker', 'istio']
:common-includes: https://raw.githubusercontent.com/OpenLiberty/guides-common/master
:source-highlighter: prettify
= Deploying microservices to Kubernetes

Explore how to deploy microservices to Kubernetes using Helm charts and manage them with kubectl.

:minikube-ip: 192.168.99.100
:kube: Kubernetes


== What you'll learn

You will learn how to deploy two microservices to a local {kube} cluster using Helm charts. This
will include building and containerizing the microservices, running the containers inside {kube}
Pods, setting up an Ingress to load balance traffic and persists sessions, establishing communicating
between Pods, scaling Deployments, automatically rolling out new updates when the microservices are
rebuilt, editing {kube} resources, and testing microservices that are deployed to {kube}.

The two microservices you will deploy are called `name` and `ping` and are provided for you under the
`start` directory. The `name` microservice simply displays a brief greeting and the name of the
container that it runs in. This makes it easy to distinguish one running instance of it from another
after its been replicated. The `ping` microservice simply "pings" the {kube} Service that encapsulates
the Pods running the `name` microservice. This demonstrates how communication can be established between
Pods inside a cluster.

You will use Minikube as your local {kube} cluster and the Helm package manager for {kube} to
deploy the two microservices. You will also use NGINX as your Ingress controller, which you will enable
as an addon in Minikube.


// =================================================================================================
// What is {kube}
// =================================================================================================

== What is {kube}?

{kube} is an open-source container orchestrator that automates many tasks involved in deploying,
managing, and scaling containerized applications.

Over the years, {kube} has become a major tool in containerized environments, all thanks to the
wide popularity of using containers for all steps of a continuous delivery pipeline.

=== Architecture

When we speak about "deploying an application to {kube}", we really mean deploying an application
to a {kube} cluster.

A typical {kube} cluster is a collection of physical or virtual machines called `Nodes` that run
containerized applications. A cluster is made up of one master `Node`, which manages the cluster, and
multiple worker `Nodes`, which run the actual application instances inside {kube} objects called `Pods`.

A `Pod` is a basic building block in a {kube} cluster. It represents a single running process which
encapsulates a container or in some scenarios multiple closely-coupled containers. `Pods` can be
replicated to scale applications and handle more traffic. From the perspective of a cluster, a set
of replicated `Pods` is still one application instance, although it might be made up of dozens of
instances of itself. A single `Pod` or a group of replicated `Pods` are managed by {kube} objects
called `Controllers`. A `Controller` handles replication, self-healing, rollout of updates, and general
management of `Pods`. Some examples of `Controllers` include `Deployments`, `StatefulSets`, and `DaemonSets`.
In this guide, you will work with `Deployments`.

A `Pod` or a group of replicated `Pods` are further grouped inside {kube} objects called `Services`,
which define a set of rules by which the `Pods` can be accessed. In a basic scenario, a {kube}
`Service` will expose a node port, which can be used together with the cluster IP address to access
the `Pods` encapsulated by the `Service`. In this guide however, you will set up an `Ingress`, which
is a {kube} object that maps external requests to the `Services` inside a cluster, as well as provides
additional functionality.

=== Why use {kube}?

Managing containers on their own can be challenging. While a few containers used for development by
a small team might not pose a problem, a large number of containers, all running various applications,
all sharing databases, all communicating with each other, and all needing to be run 24/7 will give
even a large team of experience developers a headache. {kube} is a developer's primary tool in containerized
environments. It handles scheduling, deployment, as well as mass deletion and creation of containers.
It provides update rollout capabilities on a large scale that would otherwise prove extremely tedious
to do. Imagine that you updated a Docker image, which now needs to propagate to a dozen containers.
While you could destroy and then recreate these containers, you could also issue a short one-line
command and have {kube} do all that updating for you. Of course this is just a simple example, and
there is much more underneath the iceberg that is {kube}.

// {kube} provides much other functionality
// that simply makes your development, testing, and production environments much more stable, much more
// automated, and much more powerful to use. Some of this functionality, we're hoping you can learn in
// this guide.


// =================================================================================================
// Prerequisites
// =================================================================================================

== Prerequisites

Before you begin, make sure to have the following tools installed:

- `kubernetes-cli` - a command line interface for {kube} called `kubectl`. This is your primary
tool for communicating with and managing your {kube} cluster.
- `kubernetes-helm` - a package manager for {kube} called Helm. Helm allows you to install packages
or "charts", which are sets of preconfigured {kube} resources. Installing charts is much more
convenient than creating and configuring the resources yourself.
- `minikube` - a local, single-node {kube} cluster that runs in a virtual machine.

Finally, you will need a containerization software for building containers. {kube} supports a variety
of container types and while you're not limited to any of them in particular, you should use Docker
as this is what this guide will focus on.


// =================================================================================================
// Getting Started
// =================================================================================================

include::{common-includes}/gitclone.adoc[]

// no "try what you'll build" section in this guide since it would be too long due to all setup the user will have to do.


// =================================================================================================
// Staring and preparing your cluster for Deployment
// =================================================================================================

== Starting and preparing your cluster for Deployment

To begin working with your Minikube cluster run the following command from the command line:

```
minikube start
```

This command sets up and starts a local {kube} cluster that is ready for deployment out of the box.
Later, when you no longer need you cluster, you can stop it with `minikube stop` and delete it completely
with `minikube delete`.

Next, in order to setup an Ingress, you'll need to deploy the NGINX Ingress controller, which conveniently
comes as an optional addon in Minikube. To enable this addon, run the following command:

```
minikube addons enable ingress
```

Once enabled, the Ingress controller deploys as a Pod in the `kube-system` namespace and configures
a frontend load balancer to use the rules defined in Ingress resources to route external traffic
to internal {kube} Services.

Next, install the Helm client and server by running the following command:

```
helm init
```

This command sets up `helm`, the Helm client, as well as `Tiller`, the Helm server. `Tiller` is
installed directly into your cluster and manages your chart releases (installations).

Next, add the IBM Helm chart repository to gain access to a list of our charts. This includes the
Open Liberty chart which you will use to deploy your two microservices:

```
helm repo add ibm-charts https://raw.githubusercontent.com/IBM/charts/master/repo/stable/
```

Finally, run the following command to configure the Docker CLI to use Minikube's Docker daemon.
By doing this, you will be able to interact with Minikube's Docker daemon and build new images directly
to it from your host machine:

```
# From Bash if you're on Linux or MacOS
eval $(minikube docker-env)
# From PowerShell if you're on Windows
minikube docker-env | Invoke-Expression
```

When you no longer wish to use Minikube's Docker daemon, run the following command to point back to your host:

```
# From Bash if you're on Linux or MacOS
eval $(minikube docker-env -u)
# From PowerShell if you're on Windows
minikube docker-env -u | Invoke-Expression
```


// =================================================================================================
// Building and containerizing the microservices
// =================================================================================================

== Building and containerizing the microservices

The first step of deploying to {kube} is to build your microservices and containerize them with Docker.

The starting Java project, which you can find under the `start` directory, is a multi-module Maven
project thats made up of the `name` and `ping` microservices, each residing in their own directories,
`start/name` and `start/ping`. Each of these directories also contains a Dockerfile, which is necessary
for building Docker images. If you're unfamiliar with Dockerfiles, check out our https://openliberty.io/guides/docker.html[Docker guide],
which covers Dockerfiles in depth.

If you're familiar with Maven and Docker, you might be tempted to run a Maven build first and then
use the `.war` file produced by the build to build a Docker image. While this is by no means a wrong
approach, we've setup the projects such that you can build your microservices and Docker image simultaneously
as a part of a single Maven build. This is done using the `dockerfile-maven` plugin, which automatically
picks up the Dockerfile located in the same directory as its POM file and builds a Docker image from it.
To build you microservices and Docker image, run the following command:

```
mvn package
```

The `package` goal automatically invokes the `dockerfile-maven:build` goal, which runs during the
`package` phase. This goal builds a Docker image from the Dockerfile located in the same directory
as the POM file.

During the build, you'll see various Docker messages describing what images are being downloaded and
built. If the build is successful, run `docker images` from the command line to list all local Docker
images and verify that the `name:1.0-SNAPSHOT` and `ping:1.0-SNAPSHOT` images are listed among them:

[source, role="no_copy"]
----
$ docker images
REPOSITORY                                                       TAG
ping                                                             1.0-SNAPSHOT
name                                                             1.0-SNAPSHOT
open-liberty                                                     latest
gcr.io/kubernetes-helm/tiller                                    v2.9.0
k8s.gcr.io/kube-proxy-amd64                                      v1.10.0
k8s.gcr.io/kube-controller-manager-amd64                         v1.10.0
k8s.gcr.io/kube-apiserver-amd64                                  v1.10.0
k8s.gcr.io/kube-scheduler-amd64                                  v1.10.0
quay.io/kubernetes-ingress-controller/nginx-ingress-controller   0.12.0
k8s.gcr.io/etcd-amd64                                            3.1.12
k8s.gcr.io/kube-addon-manager                                    v8.6
k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64                           1.14.8
k8s.gcr.io/k8s-dns-sidecar-amd64                                 1.14.8
k8s.gcr.io/k8s-dns-kube-dns-amd64                                1.14.8
k8s.gcr.io/pause-amd64                                           3.1
k8s.gcr.io/kubernetes-dashboard-amd64                            v1.8.1
k8s.gcr.io/kube-addon-manager                                    v6.5
gcr.io/k8s-minikube/storage-provisioner                          v1.8.0
gcr.io/k8s-minikube/storage-provisioner                          v1.8.1
k8s.gcr.io/defaultbackend                                        1.4
k8s.gcr.io/k8s-dns-sidecar-amd64                                 1.14.4
k8s.gcr.io/k8s-dns-kube-dns-amd64                                1.14.4
k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64                           1.14.4
k8s.gcr.io/etcd-amd64                                            3.0.17
k8s.gcr.io/pause-amd64                                           3.0
----

If you don't see the `name:1.0-SNAPSHOT` and `ping:1.0-SNAPSHOT` images in Minikube's Docker image
list or if they were built to your local Docker cache rather than Minikube's, then check the Maven
build log for any potential errors and make sure that your Docker CLI is configured to use Minikube's
Docker daemon and not your host's as described in <<Starting and preparing your cluster for Deployment>>.


// =================================================================================================
// Installing a Helm chart release
// =================================================================================================

== Installing a Helm chart release

Now that your Docker images are built, deploy them using the Open Liberty Helm chart.

As mentioned previously, charts are sets of {kube} resources, such as Deployments,
Services, Ingresses, etc., all configured conveniently for some purpose. In this case, that purpose
is to run microservices in Open Liberty. All resources that are installed through a chart are configurable
just like any other {kube} resources, allowing you to tweak them to your liking. All chart resources
are also deleted whenever the chart release is purged, allowing you to easily deploy a set of resources
to your cluster, configure them, and then tear them all down simultaneously when they are no longer needed.

To install a chart release, use the `helm install --name [RELEASE-NAME] [CHART] [FLAGS]` command.

First, install the Open Liberty chart for the `name` microservice:

```
helm install --name ol-name --set image.pullPolicy=IfNotPresent --set image.repository=name --set image.tag=1.0-SNAPSHOT --set ssl.enabled=false --set service.port=9080 --set service.targetPort=9080 --set ingress.enabled=true --set ingress.secureBackends=false --set ingress.rewriteTarget=/api/name --set ingress.path=/name ibm-charts/ibm-open-liberty
```

Then, for the the `ping` microservice:

```
helm install --name ol-ping --set image.pullPolicy=IfNotPresent --set image.repository=ping --set image.tag=1.0-SNAPSHOT --set ssl.enabled=false --set service.port=9080 --set service.targetPort=9080 --set ingress.enabled=true --set ingress.secureBackends=false --set ingress.rewriteTarget=/api/ping --set ingress.path=/ping ibm-charts/ibm-open-liberty
```

*Temporary instructions:*
Patch the ingresses by running the following two commands:
```
kubectl patch ingress/ol-name-ibm-open-liberty -p '{ "metadata": { "annotations": { "nginx.ingress.kubernetes.io/affinity": "cookie", "nginx.ingress.kubernetes.io/rewrite-target": "/api/name", "nginx.ingress.kubernetes.io/session-cookie-hash": "sha1", "nginx.ingress.kubernetes.io/session-cookie-name": "route" }}}'
```
```
kubectl patch ingress/ol-ping-ibm-open-liberty -p '{ "metadata": { "annotations": { "nginx.ingress.kubernetes.io/affinity": "cookie", "nginx.ingress.kubernetes.io/rewrite-target": "/api/ping", "nginx.ingress.kubernetes.io/session-cookie-hash": "sha1", "nginx.ingress.kubernetes.io/session-cookie-name": "route" }}}'
```

// On windows:
// kubectl patch ingress/ol-name-ibm-open-liberty -p "{ \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/affinity\": \"cookie\", \"nginx.ingress.kubernetes.io/rewrite-target\": \"/api/name\", \"nginx.ingress.kubernetes.io/session-cookie-hash\": \"sha1\", \"nginx.ingress.kubernetes.io/session-cookie-name\": \"route\" }}}"

Both of these chart releases will create three {kube} resource each: a Deployment for managing Pods,
a Service for defining how Pods are accessed, and an Ingress for defining how external traffic is routed
to the Service.

Each command is quite long and has a lot of flags, so let's break them down:

[cols="15, 100", options="header"]
|===
| *Flag* | *Description*
| name   | Name for the chart release.
| set    | Overrides a configuration value in the chart.
|===

Next, let's break down the parameters:

[cols="15, 15, 100", options="header"]
|===
| *Qualifier* | *Argument*     | *Description*
| image       | pullPolicy     | Image pull policy. In this case, you're using `IfNotPresent` for both
                                 microservices to have Kubelet pick up the local `name` and `ping`
                                 images rather than attempting to pull them from a public registry.
|             | repository     | Image name.
|             | tag            | Image tag. In this case, you're using `1.0-SNAPSHOT` for both microservices
                                 since its the version of your Maven project.
| ssl         | enable         | Specifies whether to use SSL. In this case, you're disabling it since
                                 both microservices are not secured. As a result, you are also using
                                 the `9080` HTTP port for the `port` and `targetPort` parameters.
| service     | port           | The port exposed by the container.
|             | targetPort     | The port that will be exposed externally by the Pod.
| ingress     | enable         | Specifies whether to create an Ingress. An Ingress is a collection of rules
                                 that enable inbound requests to reach the internal {kube} Services.
|             | secureBackends | Tells NGINX which protocol it should use to reach the backend services.
                                 HTTPS if set to `true` and HTTP otherwise. In this case, since your
                                 microservices are not secured, you're using plain HTTP.
|             | rewriteTarget  | The endpoint where the traffic will be redirected. In this case, you're
                                 using the endpoints where your microservices are served.
|             | path           | A path to which the Ingress will map a particular backend service.
|===

If you need to use additional parameters or if you would like more information on the existing parameters,
visit the official IBM chart https://github.ibm.com/IBMPrivateCloud/charts/tree/master/stable/ibm-open-liberty[GitHub repository].

Once the charts are installed, run the `kubectl get pods` command to check on the status of your Pods:

[source, role="no_copy"]
----
$ kubectl get pods
NAME                                        READY     STATUS    RESTARTS   AGE
ol-name-ibm-open-liberty-84fcb9475d-mgzjk   1/1       Running   0          55m
ol-ping-ibm-open-liberty-6cb6ffd7b6-5pp7w   1/1       Running   0          4m
----

You can also inspect the Pods in more detail by running the `kubectl describe pods [POD NAME]` command:

[source, role="no_copy"]
----
$ kubectl describe pods
Name:           ol-name-ibm-open-liberty-84fcb9475d-mgzjk
Namespace:      default
Node:           minikube/10.0.2.15
Start Time:     Tue, 22 May 2018 11:15:56 -0400
Labels:         app=ol-name-ibm-open-liberty
                Pod-template-hash=4097650318
Annotations:    <none>
Status:         Running
IP:             172.17.0.7
Controlled By:  ReplicaSet/ol-name-ibm-open-liberty-84fcb9475d
Containers:
  ibm-open-liberty:
    Container ID:   docker://fcf2bddf4640931a5c0d378f5d85d1fb1fd3b74982fecbda3267cd17d40576b6
    Image:          name:1.0-SNAPSHOT
    Image ID:       docker://sha256:71ff030f9e47db7b4513289e223fcf55e856e8b6f7e5c634e5249c5a05683016
    Port:           <none>
    Host Port:      <none>
    State:          Running
...
----

You can also issue the `get` and `describe` commands on other {kube} resources, so feel free to
to inspect all other resources created by the chart.

Wait for the Pods to be in the ready state, then access them via the Ingress that you created earlier.
If the `host` field is not specified in an Ingress, it will used your cluster's IP address. To obtain
the IP address of your Minikube cluster, run the `minikube ip` command, the IP address will be {minikube-ip}
by default. Then `curl` or visit the following URLs to access your microservices:

- https://{minikube-ip}/name/
- https://{minikube-ip}/ping/ol-name-ibm-open-liberty

The first URL will return a brief greeting followed by the name of the Pod that the `name` microservice
runs in. The second URL will return `pong` if it received a good response from the `ol-name-ibm-open-liberty`
{kube} Service. Visiting https://{minikube-ip}/ping/{kube-service} in general will return either
a good or a bad response depending on whether `kube-service` is a valid {kube} Service that can be accessed.

There is a lot going when you send a request, so let's break it down. When you issue a request to either
URL, the NGINX Ingress controller sees the request arrive at the apiserver's `/ingresses` endpoint and
re-routes this request appropriately using the set of rules defined in the appropriate Ingress resource.
This set of rules states that all requests made to the https://{minikube-ip}/name/ URL are to be mapped
to the `/api/name` endpoint of the {kube} Service running `name` Pods, and similarly for the https://{minikube-ip}/ping/
URL. Once a request arrives at a {kube} Service, the Service uses its own set of rules to map this
request to a Pod, which then sends back a response, which the Service passes back to the client.

=== Alternative: deploying the microservices without an Ingress

You can choose to omit the Ingress entirely and install your charts without it. Ingress is disabled
by default, so all you need to do is omit all Ingress-related parameters.

Once your chart is installed, each {kube} Service will chose a random `nodePort` within a predefined
range to expose. This will be the port number through which you can access the Service encapsulating
your Pods. To find this port number, run the `kubectl get services` command. The `nodePort` will be the
port number that the `9080` port maps to. In the following case the `nodePort` is `31249`:

[source, role="no_copy"]
----
$ kubectl get services
NAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes                   ClusterIP   10.96.0.1        <none>        443/TCP          17h
ol-name-ibm-open-liberty     NodePort    10.101.49.188    <none>        9080:31249/TCP   2s
----

The `name` microservice can then be accessed by `curling` or visiting the http://{minikube-ip}:31249/api/name URL.

The disadvantage of not having an Ingress is that {kube} doesn't have a practical load balancer.
In fact, traffic is balanced randomly by default. This could prove extremely inconvenient when by
chance, all traffic is routed to a single Pod, overloading it and causing it to crash.


// =================================================================================================
// Scaling a Deployment
// ================================================================================================

== Scaling a Deployment

To really make use of load balancing and session persistence that comes with your Ingress, you need
to scale your Deployments. When you scale a Deployment, you replicate its Pods, creating more running
instances of your applications. Scaling is one of the primary advantages of {kube} as it allows to
accommodate more traffic, and descale them to free up resources when the traffic decreases.

As an example, scale the `name` Deployment to 3 Pods. This can be done by running the following command:

```
kubectl scale deployment/ol-name-ibm-open-liberty --replicas=3
```

Next, run the `kubectl get pods` command and wait until your two new Pods are in the ready state:

[source, role="no_copy"]
----
$ kubectl get Pods
NAME                                        READY     STATUS              RESTARTS   AGE
ol-name-ibm-open-liberty-84fcb9475d-grvw9   0/1       ContainerCreating   0          1s
ol-name-ibm-open-liberty-84fcb9475d-hgvg2   1/1       Running             0          44s
ol-name-ibm-open-liberty-84fcb9475d-rctgp   0/1       ContainerCreating   0          1s
ol-ping-ibm-open-liberty-6cb6ffd7b6-fhchz   1/1       Running             0          34s
----

Then `curl` or visit the https://{minikube-ip}/name/ URL. Each unique session that you open to this URL
will display a different Pod name, one for each of your three running Pods. Notice also that no matter
how many unique sessions you open, all your traffic will be balanced perfectly among the three Pods.
This is your Ingress controller's load balancer at work.

If you attempted to open multiple session in a simple browser window, you also saw how your Ingress
controller persists session. In other words, each session opened in a plain browser window will result
in you connecting to the same Pod and hence seeing the same Pod name displayed. This is because of
session persistence, meaning that your Ingress controller is passing your requests to the same backend
service each time in order to persistent any data that you might have created during your first session.
Session persistence can be configured and disabled from your Ingress resource.

The Open Liberty helm chart also supports automatic scaling, which you can enable by setting the
`autoscaling.enabled` parameter to true when installing the chart. See our official IBM chart https://github.ibm.com/IBMPrivateCloud/charts/tree/master/stable/ibm-open-liberty[GitHub repository]
for more information on this parameter.


// =================================================================================================
// Rolling out Deployment updates
// =================================================================================================

== Rolling out Deployment updates

Whenever you make code changes and rebuild your Docker images, you'll need to update your {kube}
Deployments with the new image versions for the new code changes to be picked up.

As an example, make a small change to the `name` microservice and then update the `ol-name-ibm-open-liberty`
Deployment thats already installed in your cluster.

First, change the greeting message in `name/src/main/java/io/openliberty/guides/name/NameResource.java`
file from "Hello!" to "Greetings!" :

[source, Java]
----
package io.openliberty.guides.name;

import javax.enterprise.context.RequestScoped;
import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.core.MediaType;

@RequestScoped
@Path("/")
public class NameResource {

    @GET
    @Produces(MediaType.TEXT_PLAIN)
    public String getContainerName() {
        return "Greetings! I'm container " + System.getenv("HOSTNAME");
    }

}
----

Next, specify a new version for your Maven project. This way, when your microservice rebuilds, a
Docker image with a new tag will be built alongside it. To change the Maven project version, run the
following command from the `start` directory:

```
mvn versions:set -DnewVersion=1.1-SNAPSHOT
```

This command will upgrade the parent project version from `1.0-SNAPSHOT` to `1.1-SNAPSHOT` and
propagate this change automatically to the child projects:

[source, role="no_copy"]
----
...
[INFO] --- versions-maven-plugin:2.5:set (default-cli) @ kube-demo ---
[INFO] Searching for local aggregator root...
[INFO] Local aggregation root: /Users/foo/Documents/repos/guides/wip/draft-guide-kubernetes/finish
[INFO] Processing change of io.openliberty.guides:kube-demo:1.0-SNAPSHOT -> 1.1-SNAPSHOT
[INFO] Processing io.openliberty.guides:kube-demo
[INFO]     Updating project io.openliberty.guides:kube-demo
[INFO]         from version 1.0-SNAPSHOT to 1.1-SNAPSHOT
[INFO]
[INFO] Processing io.openliberty.guides:name
[INFO]     Updating parent io.openliberty.guides:kube-demo
[INFO]         from version 1.0-SNAPSHOT to 1.1-SNAPSHOT
[INFO]
[INFO] Processing io.openliberty.guides:ping
[INFO]     Updating parent io.openliberty.guides:kube-demo
[INFO]         from version 1.0-SNAPSHOT to 1.1-SNAPSHOT
...
----

Next, navigate to the `name` directory and run the `mvn clean package` command to rebuild the `name`
microservice. Then run the `docker images` command and verify that a new `name:1.1-SNAPSHOT` image was
created:

[source, role="no_copy"]
----
$ docker images
REPOSITORY                                                       TAG
name                                                             1.1-SNAPSHOT
ping                                                             1.0-SNAPSHOT
name                                                             1.0-SNAPSHOT
----

To deploy this new image into your cluster, you can either install a new chart release, specifying
the new image version in the `image.tag` parameter, or you can upgrade the existing `ol-name-ibm-open-liberty`
Deployment thats part of your `ol-name` release.

Installing a new chart release is done using the same the same `helm install` command as before. To
update an existing release, you need to update the image tag in the Deployment in order for it to point
to your new image version. To do this, run the following command:

```
kubectl set image deployment/ol-name-ibm-open-liberty ibm-open-liberty=name:1.1-SNAPSHOT --record
```

When you change the image tag, {kube} automatically creates new Pods that run this new image. {kube}
also keeps some of the old Pods alive until enough of the new Pods are running.

[source, role="no_copy"]
----
$ kubectl get Pods
NAME                                        READY     STATUS        RESTARTS   AGE
ol-name-ibm-open-liberty-84fcb9475d-hgvg2   1/1       Terminating   1          19h
ol-name-ibm-open-liberty-84fcb9475d-rctgp   1/1       Running       1          19h
ol-name-ibm-open-liberty-9db5b8b65-5ncgt    1/1       Running       0          28s
ol-name-ibm-open-liberty-9db5b8b65-88psh    0/1       Running       0          28s
ol-name-ibm-open-liberty-9db5b8b65-cxn5q    0/1       Running       0          1s
ol-ping-ibm-open-liberty-6cb6ffd7b6-fhchz   1/1       Running       1          19h
----

When all of the new Pods are in the ready state and all of the old Pods terminate, `curl` or visit
the https://{minikube-ip}/name/ URL and verify that the greeting has changed.

=== Alternative: deploying new images without changing project versions

If for any reason you choose not to update the image tag, then you can force a Deployment to pickup
the new image by manually deleting all of its Pods. Because Deployments "self-heal", three new Pods
will be created to replace the ones you've destroyed, and because these Pods are created from scratch,
they will run the new image. To do this, use the `kubectl delete pods` command. For example:

```
kubectl delete pods -l app=ol-name-ibm-open-liberty
```

When all of the new Pods are in the ready state, `curl` or visit the https://{minikube-ip}/name/ URL.

=== Automating with Maven

To make the rollout of updates easier, we've created a `update-deployment` profile in the POM files
of each microservice. This profile uses the Maven exec plugin to automatically run the `kubectl set image`
command. This command was configured to use the Maven project artifactId as the image name and the
project version as the image tag. Feel free to tweak this configuration to your liking.

Using the profile is optional, so you can always rebuild the microservices without updating the deployment.
Also note that you must install a chart release before you use the profile as there will be nothing
to update since your Deployment would not yet exist. To invoke the profile when rebuilding, use the
`-P` flag followed by the profile name:

```
mvn clean package -P update-deployment
```

If new updates were made to a Deployment, you will see a brief message in the Maven build log, like so:

[source, role="no_copy"]
----
...
[INFO] --- exec-maven-plugin:1.6.0:exec (update-kubernetes-deployment) @ name ---
deployment.apps "ol-name-ibm-open-liberty" image updated
...
----

If no updates were made to a Deployment, then no special messages will show. If you did make code changes
and this happens, make sure that you updated your image tag, as {kube} will not update a deployment
thats already using the same image version.


// =================================================================================================
// Rolling back Deployments to previous revisions
// =================================================================================================

== Rolling back Deployments to previous revisions

If you rolled out an unstable Deployment update, then you can revert the Deployment back to a older
revision by viewing its revision history:

[source, role="no_copy"]
----
$ kubectl rollout history deployment/ol-name-ibm-open-liberty
deployments "ol-name-ibm-open-liberty"
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment/ol-name-ibm-open-liberty ibm-open-liberty=name:1.1-SNAPSHOT --record=true
----

To undo the greeting messages changes that you made in the current rollout and revert the Deployment
back to its previous version, run the following command:

```
kubectl rollout undo deployment/ol-name-ibm-open-liberty
```

{kube} will terminate any existing Pods under this version of the Deployment and create new ones from
the previous revision.

If you need to revert back to a specific revision, use the `--to-revision` flag followed by the revision number:

```
kubectl rollout undo deployment/ol-name-ibm-open-liberty --to-revision=1
```


// =================================================================================================
// Testing microservices that are running on {kube}
// =================================================================================================

== Testing microservices that are running on {kube}

A few tests are included for you to test the basic functionality of the microservices. If a test failure
occurs, then you might have introduced a bug into the code. To run the tests, wait for all Pods to be
in the ready state, then run the `mvn verify` command with the following list of properties:

- `cluster.ip` -  IP address of your Minikube cluster, `{minikube-ip}` by default.
- `name.ingress.path` - Ingress path of the `name` microservice, `/name` by default.
- `ping.ingress.path` - Ingress path of the `ping` microservice, `/ping` by default.
- `name.kube.service` - Name of the {kube} Service wrapping the `name` Pods, `ol-name-ibm-open-liberty` by default.
- `name.node.port` - Node port of the {kube} Service wrapping the `name` Pods. Set this if you didn't
setup an Ingress.
- `ping.node.port` - Node port of the {kube} Service wrapping the `ping` Pods. Set this if you didn't
setup an Ingress.

For example:

```
mvn verify -Ddockerfile.skip=true
```

This command will run integration tests with all default properties. The `dockerfile.skip` parameter
is set to `true` in order to skip building a new Docker image. If the tests pass, you'll see an output
similar to the following for each service respectively:

Alternatively, if you didn't setup an Ingress, you can use set the `name.node.port` and `ping.node.port`
properties like so to make requests to your {kube} Services instead:

```
mvn verify -Dname.node.port=<name-node-port> -Dping.node.port=<ping-node-port> -Ddockerfile.skip=true
```

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.name.NameEndpointTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.673 sec - in it.io.openliberty.guides.name.NameEndpointTest

Results :

Tests run: 1, Failures: 0, Errors: 0, Skipped: 0
----

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.ping.PingEndpointTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.222 sec - in it.io.openliberty.guides.ping.PingEndpointTest

Results :

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0
----


// =================================================================================================
// Editing {kube} resources
// =================================================================================================

== Optional: Editing {kube} resources

While you don't need to edit any of the {kube} resources in this guide, it might be helpful for
you to know how this is done for any future projects that you have in mind.

To make changes to a {kube} resource, you can either edit that resource's whole YAML file in a text
editor or update particular parts of the YAML with the `kubectl` command.

=== Direct editing

Resource YAML files can be directly edited from the {kube} dashboard or a text editor of your choice.

To edit a resource from the Minikube dashboard, run the `minikune dashboard` command to open the Minikube
dashboard, and then use the left navigational panel to select which resources to edit.

To open and edit a resource in a text editor, run the `kubectl edit (RESOURCE/NAME | -f FILENAME) [options]`
command, specifying the resource type and its name. Once you save your changes, they will be automatically
picked up and applied to your resource.

If you didn't create resources through charts, but rather implemented and created them yourself from
YAML files by using the `kubectl create` command, then you can edit these YAMLs directly and then
reapply them by running the `kubectl apply -f [FILENAME] options` command.

To familiarize yourself with resource editing, edit the `ol-name-ibm-open-liberty` Ingress and change
the `path` field in the `spec` object from `/name` to `/myname`. Use any editing method you prefer:

[source, YAML]
----
{
  "kind": "Ingress",
  "apiVersion": "extensions/v1beta1",
  "metadata": {
    "name": "ol-name-ibm-open-liberty",
    "namespace": "default",
    "selfLink": "/apis/extensions/v1beta1/namespaces/default/ingresses/ol-name-ibm-open-liberty",
    "uid": "18353cff-5fc5-11e8-af4d-08002784f87f",
    "resourceVersion": "64202",
    "generation": 1,
    "creationTimestamp": "2018-05-25T02:41:15Z",
    "labels": {
      "app": "ol-name-ibm-open-liberty",
      "chart": "ibm-open-liberty-1.2.0",
      "heritage": "Tiller",
      "release": "ol-name"
    },
    "annotations": {
      "kubernetes.io/ingress.class": "nginx",
      "nginx.ingress.kubernetes.io/affinity": "cookie",
      "nginx.ingress.kubernetes.io/rewrite-target": "/api/name",
      "nginx.ingress.kubernetes.io/session-cookie-hash": "sha1",
      "nginx.ingress.kubernetes.io/session-cookie-name": "route"
    }
  },
  "spec": {
    "rules": [
      {
        "http": {
          "paths": [
            {
              "path": "/myname",
              "backend": {
                "serviceName": "ol-name-ibm-open-liberty",
                "servicePort": 9080
              }
            }
          ]
        }
      }
    ]
  },
  "status": {
    "loadBalancer": {
      "ingress": [
        {
          "ip": "10.0.2.15"
        }
      ]
    }
  }
}
----

When you're done editing, visit the new Ingress endpoint to verify that your changes have been applied.


=== Indirect editing

Indirect editing of resources can be done by using various `kubectl` commands. If you recall,
you've already done this when you ran `kubectl set image` to update the image used by your Deployment
to a newer version. `kubectl set` is convenient, but its limited to a small set of fields that you can
change. Sometimes you might need to change parts of a resource that the `kubectl set` command simply
doesn't cover. In those cases and as an alternative to the `kubectl set` command, you can use the
`kubectl patch (-f FILENAME | TYPE NAME) -p PATCH [options]` command to make updates using strategic
merging. The `kubectl patch` command works by supplying a piece of config in a form of a JSON or a YAML
that matches another piece of similar config in the resource and overrides the fields that don't match.
For example, to change the Ingress path from `/name` to `/myname` as in the previous section, run the
following command:

```
kubectl patch ingress/ol-name-ibm-open-liberty -p '{"spec": {"rules": [{"http": {"paths": [{"path": "/myname", "backend": {"serviceName": "ol-name-ibm-open-liberty", "servicePort": 9080}}]}}]}}'
```

You can also point to a JSON or a YAML file that contains your config instead of writing it out in
your command line.

{kube} will match your config pattern against the `spec` object defined in the `ol-name-ibm-open-liberty`
Ingress and override the `path` field.


// =================================================================================================
// finish
// =================================================================================================

== Great work! You're done!

You have just deployed two microservices to {kube} using Helm charts. You then scaled a microservice,
rolled out deployment updates, learned how to edit {kube} resources, and ran integration tests against
miroservices that are running in a {kube} cluster.

// uncomment this when Istio guide is released
//Feel free to check out our https://github.com/OpenLiberty/guide-istio[Istio guide], which builds on top of what you learned here.

// Include the below from the guides-common repo to tell users how they can contribute to the guide
include::{common-includes}/finish.adoc[]

// DO NO CREATE ANYMORE SECTIONS AT THIS POINT
// Related guides will be added in automatically here if you included them in ":page-related-guides"
